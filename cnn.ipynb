{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "3x8p12r5Dfff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tensorflow"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smF0dd13Je0U",
        "outputId": "eb3b42e4-0c52-402e-c271-28be634cf14b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from datasets import Dataset, DatasetDict\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "DbE6XIm7De95"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "478yFhYmAdG4"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"Marxulia/asl_sign_languages_alphabets_v03\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Exploration**"
      ],
      "metadata": {
        "id": "SelYwIuPESAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total samples:\", len(dataset['train']))\n",
        "print(f\"Categories: \", dataset['train'].features['label'].names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m2pI-etEWFT",
        "outputId": "bb078f2b-ac9b-4e13-a2a4-342f5202ef05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 10873\n",
            "Categories:  ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets (e.g., 80% train, 20% test)\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "    dataset['train']['image'], dataset['train']['label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create a DatasetDict to hold both the train and test sets\n",
        "dataset = DatasetDict({\n",
        "    'train':  Dataset.from_dict({'image': train_images, 'label': train_labels}),\n",
        "    'test': Dataset.from_dict({'image': test_images, 'label': test_labels})\n",
        "})"
      ],
      "metadata": {
        "id": "tobEFMDQNDAJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqXg9pTKQuFy",
        "outputId": "fad3f6fb-66a0-4886-e0d2-6f746f449e80"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['image', 'label'],\n",
            "        num_rows: 8698\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['image', 'label'],\n",
            "        num_rows: 2175\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**"
      ],
      "metadata": {
        "id": "Wd70quS6WGqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(example):\n",
        "    image = example['image']\n",
        "    image = tf.convert_to_tensor(np.array(image), dtype=tf.float32)\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    image = image / 255.0\n",
        "    label = example['label']\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "-dJW81HxEjog"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TensorFlow dataset using tf.data with preprocessing\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: (preprocess_image(example) for example in dataset['train']),\n",
        "    output_signature=(tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32), tf.TensorSpec(shape=(), dtype=tf.int64))\n",
        ")\n",
        "\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: (preprocess_image(example) for example in dataset['test']),\n",
        "    output_signature=(tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32), tf.TensorSpec(shape=(), dtype=tf.int64))\n",
        ")"
      ],
      "metadata": {
        "id": "0nNBT2vLFt2X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.batch(batch_size=64, drop_remainder=True).shuffle(5000).prefetch(tf.data.experimental.AUTOTUNE).repeat()\n",
        "val_dataset = val_dataset.batch(batch_size=64, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE).repeat()"
      ],
      "metadata": {
        "id": "YdN26mVbF-V0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect a few batches of data\n",
        "for image, label in train_dataset.take(2):  # Get the first batch\n",
        "    print(\"Image shape:\", image.shape)\n",
        "    print(\"Label:\", label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8Tv0jtxaQNc",
        "outputId": "a71d5ca3-e59c-4560-be7d-0b325be48d26"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 224, 224, 3)\n",
            "Label: tf.Tensor(\n",
            "[ 7 16  4  4  1 14 20 24 25 20 22  0  7  4  0 16  7  2 18 16  4 15  0 19\n",
            " 14 21 23  6 21 25  7 24], shape=(32,), dtype=int64)\n",
            "Image shape: (32, 224, 224, 3)\n",
            "Label: tf.Tensor(\n",
            "[ 7 16 24 15 12 22 14  0  3 24 10  8 17 23  3  9  3 10 17  3  6 20 25 17\n",
            " 13 10 13 12  8  1  2 14], shape=(32,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "\n",
        "        # Convolutional Layer 1\n",
        "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Convolutional Layer 2\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Convolutional Layer 3\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Flatten layer to convert 2D matrix to 1D vector\n",
        "        layers.Flatten(),\n",
        "\n",
        "        # Fully connected (Dense) layer\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # Fully connected (Dense) layer\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # Fully connected (Dense) layer\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # Output layer (classification)\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',  # For integer labels\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the CNN model\n",
        "cnn_model = create_cnn_model(input_shape=(224, 224, 3), num_classes=26)"
      ],
      "metadata": {
        "id": "5VwnvtpDRxxf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=40,\n",
        "    steps_per_epoch=len(dataset['train']) // 64,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=len(dataset['test']) // 64\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzyGXMG0R_dR",
        "outputId": "314c2144-4b07-45a0-b290-9c176363166a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 112ms/step - accuracy: 0.0387 - loss: 3.3182 - val_accuracy: 0.0331 - val_loss: 3.2587\n",
            "Epoch 2/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 95ms/step - accuracy: 0.0364 - loss: 3.2584 - val_accuracy: 0.0364 - val_loss: 3.2591\n",
            "Epoch 3/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 94ms/step - accuracy: 0.0351 - loss: 3.2583 - val_accuracy: 0.0308 - val_loss: 3.2594\n",
            "Epoch 4/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 89ms/step - accuracy: 0.0380 - loss: 3.2579 - val_accuracy: 0.0340 - val_loss: 3.2590\n",
            "Epoch 5/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 85ms/step - accuracy: 0.0396 - loss: 3.2576 - val_accuracy: 0.0354 - val_loss: 3.2571\n",
            "Epoch 6/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 87ms/step - accuracy: 0.0463 - loss: 3.2528 - val_accuracy: 0.0499 - val_loss: 3.2482\n",
            "Epoch 7/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 87ms/step - accuracy: 0.0598 - loss: 3.2266 - val_accuracy: 0.0630 - val_loss: 3.2231\n",
            "Epoch 8/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 86ms/step - accuracy: 0.0755 - loss: 3.1759 - val_accuracy: 0.0718 - val_loss: 3.2126\n",
            "Epoch 9/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 98ms/step - accuracy: 0.0923 - loss: 3.1035 - val_accuracy: 0.0798 - val_loss: 3.1798\n",
            "Epoch 10/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 88ms/step - accuracy: 0.1182 - loss: 2.9921 - val_accuracy: 0.0951 - val_loss: 3.1433\n",
            "Epoch 11/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 88ms/step - accuracy: 0.1605 - loss: 2.8234 - val_accuracy: 0.1017 - val_loss: 3.1668\n",
            "Epoch 12/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 90ms/step - accuracy: 0.2161 - loss: 2.5848 - val_accuracy: 0.1101 - val_loss: 3.2766\n",
            "Epoch 13/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 91ms/step - accuracy: 0.2856 - loss: 2.3364 - val_accuracy: 0.1124 - val_loss: 3.3048\n",
            "Epoch 14/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 88ms/step - accuracy: 0.3654 - loss: 2.0259 - val_accuracy: 0.1236 - val_loss: 3.4626\n",
            "Epoch 15/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 88ms/step - accuracy: 0.4507 - loss: 1.7628 - val_accuracy: 0.1273 - val_loss: 3.5606\n",
            "Epoch 16/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 86ms/step - accuracy: 0.5155 - loss: 1.5143 - val_accuracy: 0.1278 - val_loss: 3.8021\n",
            "Epoch 17/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 88ms/step - accuracy: 0.6089 - loss: 1.2493 - val_accuracy: 0.1343 - val_loss: 4.0677\n",
            "Epoch 18/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 87ms/step - accuracy: 0.6644 - loss: 1.0807 - val_accuracy: 0.1297 - val_loss: 4.3417\n",
            "Epoch 19/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 86ms/step - accuracy: 0.7206 - loss: 0.9000 - val_accuracy: 0.1264 - val_loss: 4.2680\n",
            "Epoch 20/20\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 92ms/step - accuracy: 0.7442 - loss: 0.8303 - val_accuracy: 0.1343 - val_loss: 4.6454\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d37acbe3bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}